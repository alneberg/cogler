#!/usr/bin/env python
import sys
from BCBio import GFF
import argparse
from cogler.io import read_blast_output, read_gff_file, read_clustering_file, read_markers_file
from collections import defaultdict

def get_records_from_file(queries, cog_file):
    # Read a simple tsv file with two columns, cddid and cogid respectively.
    with open(cog_file, 'r') as cf:
        records = dict([(row.split('\t')[0], {'Accession': row.split('\t')[1].strip()}) for row in cf.readlines()])
    return records

def main(args):

    RPSBLAST_SCOVS_THRESHOLD = args.scovs_threshold
    RPSBLAST_PIDENT_THRESHOLD = args.pident_threshold

    records, sseq_ids = read_blast_output(args.blastoutfile)

    if args.cdd_cog_file:
        # Retrieve the cog accession number from file
        cogrecords = get_records_from_file(sseq_ids, args.cdd_cog_file)
    else:
        # Retrieve the cog accession number from ncbi
        cogrecords_l = get_records_from_cdd(sseq_ids, args.email)
        cogrecords = {}
        for rec in cogrecords_l:
            cogrecords[rec['Id']] = rec


    # If a gff file is given, the contig ids will be fetched from this.
    if args.gfffile:
        featureid_locations = read_gff_file(args.gfffile)
   
    features_per_contig = defaultdict(list)

    for record_d in records:
        pident_above_threshold = record_d['pident'] >= RPSBLAST_PIDENT_THRESHOLD

        # A certain fraction of the cog should be covered to avoid the same cog 
        # to be counted twice in the case when a cog is split across two or more contigs.
        alignment_length_in_subject = abs(record_d['send'] - record_d['sstart']) + 1
        percent_seq_covered = (alignment_length_in_subject / record_d['slen']) * 100.0
        seq_cov_above_threshold =  percent_seq_covered >= RPSBLAST_SCOVS_THRESHOLD
        
        if pident_above_threshold and seq_cov_above_threshold:
            cog_accession = cogrecords[record_d['sseqid'].split('|')[2]]['Accession']
            if args.gfffile:
                contig = featureid_locations[record_d['qseqid']]
            else:
                contig = "".join(record_d['qseqid'].split(args.separator)[:-1])
            features_per_contig[contig].append(cog_accession)

    # Load clustering
    clusters, contigs_per_cluster = read_clustering_file(args.cluster_file)
    clusters.sort()

    # Load markers
    markers = read_markers_file(args.marker_file)
    markers.sort()

    # print header
    print "\t".join(["Cluster", "Contigs", "Num_contigs"] + markers)

    # Per cluster, count the number of features
    for cluster in clusters:
        contigs = contigs_per_cluster[cluster]
        contigs.sort()
        counts = [cluster, "|".join(contigs), str(len(contigs))]

        for marker in markers:
            count = 0
            for contig in contigs:
                for feature in features_per_contig[contig]:
                    if feature == marker:
                        count += 1
            counts.append(str(count))
        print "\t".join(counts)

if __name__ == "__main__":
   parser = argparse.ArgumentParser()
   parser.add_argument('-b', '--blastoutfile', required=True,
           help=('Output of rpsblast run, assumed to be in tabular format whith '
               'columns: qseqid sseqid evalue pident score qstart qend sstart send length slen. '
               'The contigs ids are assumed to be recoverable by removing the last underscore '
               'and the characters following it from the qseqid column.' ))
   parser.add_argument('-g', '--gfffile',
           help=('GFF file generated by e.g. prodigal '
           'only needed if the contig names are not recoverable from the '
           'blast output file.'))
   parser.add_argument('-c', '--cluster_file', required=True,
           help=('Clustering file from concoct execution.'))
   parser.add_argument('-m', '--marker_file', required=True,
           help=('File containing a list of genes that will be used as marker genes'))
   parser.add_argument('-s', '--scovs-threshold', type=float, default=50.0,
           help='Threshold covered in percent, default=50.0')
   parser.add_argument('-p', '--pident-threshold', type=float, default=0.0,
           help='Threshold identity in percent, default=0.0')
   parser.add_argument('--cdd_cog_file',
           help = ('Supply a cdd to cog mapping file in a tsv format '
           'to take precedence over eutils fetching of name. '
           'Useful if running this script in parallel, since '
           'NCBI eutils has a limit on the number of requests per '
           'time unit you can make.'))
   parser.add_argument('--separator', default="_",
           help=('Character that is used to separate the contig id from the '
                 'protein identifier. Everything before the last occurence ' 
                 'of this character will be used as the contig id. Default '
                 'value is "_"'))
   args = parser.parse_args()

   main(args)
